{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of verbs found in `cleaned_svo`:\n",
      "274\n",
      "Number of rows in the data: 12733\n",
      "Number of rows in the dataset after filtering out verbs with less than `375` SVO triples:\n",
      "9407\n"
     ]
    }
   ],
   "source": [
    "model_name = '32' # '32' for ViT-B/32, '14' for ViT-L/14\n",
    "triples_threshold = 375\n",
    "\n",
    "cleaned_svo_path = Path('cleaned_svo')\n",
    "existing_verbs = [] # list of verbs that we have enough SVO triples for\n",
    "for f in cleaned_svo_path.glob('*.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    if len(df) >= triples_threshold:\n",
    "        existing_verbs.append(f.stem)\n",
    "        \n",
    "data = pd.read_csv('vl_checklist.csv', index_col=0)\n",
    "\n",
    "print(f'Number of verbs found in `{cleaned_svo_path}`:\\n{len(existing_verbs)}')\n",
    "print(f'Number of rows in the data: {len(data)}')\n",
    "\n",
    "# filter out the entries whose positive verb or negative verb is not in the existing verbs\n",
    "data = data[data['pos_verb'].isin(existing_verbs) & data['neg_verb'].isin(existing_verbs)]\n",
    "print(f'Number of rows in the dataset after filtering out verbs with less than `{triples_threshold}` SVO triples:\\n{len(data)}')\n",
    "\n",
    "# make sure that all verbs in the dataset can be handled by VerbCLIP\n",
    "assert data['pos_verb'].isin(existing_verbs).all()\n",
    "assert data['neg_verb'].isin(existing_verbs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_verb_lemma'] = data['pos_verb']\n",
    "data['neg_verb_lemma'] = data['neg_verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load use pre-computed embeddings\n",
    "txt_emb_path = Path(f'embeddings/txt_emb_{model_name}.pkl')\n",
    "img_emb_path = Path(f'embeddings/img_emb_{model_name}.pkl')\n",
    "txt_emb_dict = pickle.load(open(txt_emb_path, 'rb'))\n",
    "img_emb_dict = pickle.load(open(img_emb_path, 'rb'))\n",
    "\n",
    "def encode_text(text):\n",
    "    return np.array(txt_emb_dict[text])\n",
    "\n",
    "def encode_image(image_name):\n",
    "    return np.array(img_emb_dict[image_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings in real time\n",
    "# Note: the actual images are needed to compute their embeddings\n",
    "# However, the images not available in this repository due to copyright issues\n",
    "\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# device = 'cpu'\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# def encode_text(text):\n",
    "#     inputs = clip.tokenize([text]).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         return model.encode_text(inputs).squeeze().cpu()\n",
    "\n",
    "# img_folder = Path('images')\n",
    "# def encode_image(image_name):\n",
    "#     image = Image.open(img_folder / image_name).convert('RGB')\n",
    "#     inputs = preprocess(image_name).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         return model.encode_image(image).squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the verb matrices computed using the `build_matrices.ipynb` notebook\n",
    "# in the root directory of this repository\n",
    "rel_mat_path = Path(f'embeddings/rel_matrices_{model_name}.pkl')\n",
    "rel_mat_norm_path = Path(f'embeddings/rel_matrices_norm_{model_name}.pkl')\n",
    "reg_mat_subj_path = Path(f'embeddings/reg_subj_matrices_{model_name}.pkl')\n",
    "reg_mat_obj_path = Path(f'embeddings/reg_obj_matrices_{model_name}.pkl')\n",
    "\n",
    "rel_mat_dict = pickle.load(open(rel_mat_path, 'rb'))\n",
    "rel_mat_norm_dict = pickle.load(open(rel_mat_norm_path, 'rb'))\n",
    "reg_mat_subj_dict = pickle.load(open(reg_mat_subj_path, 'rb'))\n",
    "reg_mat_obj_dict = pickle.load(open(reg_mat_obj_path, 'rb'))\n",
    "\n",
    "def get_rel_mat(verb, norm=False):\n",
    "    if norm:\n",
    "        return rel_mat_norm_dict[verb]\n",
    "    else:\n",
    "        return rel_mat_dict[verb]\n",
    "    \n",
    "def get_reg_mat(verb, part, norm=False):\n",
    "    if part == 'subj':\n",
    "        return reg_mat_subj_dict[verb]\n",
    "    elif part == 'obj':\n",
    "        return reg_mat_obj_dict[verb]\n",
    "    else:\n",
    "        raise ValueError(f'part should be either subj or obj, got {part}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_scores(pos_scores, neg_scores):\n",
    "    pos_scores = np.array(pos_scores)\n",
    "    neg_scores = np.array(neg_scores)\n",
    "    return (pos_scores > neg_scores).mean()\n",
    "\n",
    "def normalise(vec):\n",
    "    return vec / np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_methods(data, matrix_method, **options):\n",
    "    \"\"\"\n",
    "    The available options are:\n",
    "\n",
    "    - normalize_caption: whether to normalise the caption embeddings\n",
    "    - normalize_parts: whether to normalise the embeddings of the parts of the sentence\n",
    "    - normalize_compo: whether to normalise the entire compositional part.\n",
    "\n",
    "    - normalize_rel: whether to normalise the relational verb matrix by the number of SVO\n",
    "                     triples used to build it. Only used for the Relational `rel` method.\n",
    "    - normalize_kron: whether to use the normalise verb vector in the Kronecker product,\n",
    "                      only used for the Kronecker `kron` method.\n",
    "    - normalize_reg: whether to use normalised embeddings for the calculation of the\n",
    "                     Regression `reg` method.\n",
    "\n",
    "    - cos_sim: whether to use cosine similarity or dot product for the calculation of the scores.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), leave=True, position=1):\n",
    "        text_data = {'pos_verb': row['pos_verb'], 'neg_verb': row['neg_verb'], \n",
    "                     'subject': row['subject'], 'object': row['object'], \n",
    "                     'pos_caption': row['pos_caption'], 'neg_caption': row['neg_caption']}\n",
    "        text_embs = {k: encode_text(v) for k, v in text_data.items()}\n",
    "        image_embs = encode_image(row['image_id'])\n",
    "       \n",
    "        # prepare verb matrices \n",
    "        if matrix_method == 'kron':\n",
    "            pos_verb_mat = np.outer(text_embs['pos_verb'], text_embs['pos_verb'])\n",
    "            neg_verb_mat = np.outer(text_embs['neg_verb'], text_embs['neg_verb'])\n",
    "            if options.get('normalize_kron', False):\n",
    "                pos_verb_mat = pos_verb_mat / np.linalg.norm(text_embs['pos_verb'])**2\n",
    "                neg_verb_mat = neg_verb_mat / np.linalg.norm(text_embs['neg_verb'])**2\n",
    "        elif matrix_method == 'rel':\n",
    "            pos_verb_mat = get_rel_mat(row['pos_verb'], norm=options.get('normalize_rel', False))\n",
    "            neg_verb_mat = get_rel_mat(row['neg_verb'], norm=options.get('normalize_rel', False))\n",
    "        elif matrix_method == 'reg':\n",
    "            pos_verb_mat_obj = get_reg_mat(row['pos_verb'], norm=options.get('normalize_reg', False), part='obj')\n",
    "            neg_verb_mat_obj = get_reg_mat(row['neg_verb'], norm=options.get('normalize_reg', False), part='obj')\n",
    "            pos_verb_mat_subj = get_reg_mat(row['pos_verb'], norm=options.get('normalize_reg', False), part='subj')\n",
    "            neg_verb_mat_subj = get_reg_mat(row['neg_verb'], norm=options.get('normalize_reg', False), part='subj')\n",
    "            \n",
    "        if options.get('normalize_caption', False):\n",
    "            text_embs['pos_caption'] = normalise(text_embs['pos_caption'])\n",
    "            text_embs['neg_caption'] = normalise(text_embs['neg_caption'])\n",
    "            \n",
    "        if options.get('normalize_parts', False):\n",
    "            text_embs['pos_verb'] = normalise(text_embs['pos_verb'])\n",
    "            text_embs['neg_verb'] = normalise(text_embs['neg_verb'])\n",
    "            text_embs['subject'] = normalise(text_embs['subject'])\n",
    "            text_embs['object'] = normalise(text_embs['object'])\n",
    "            \n",
    "        if matrix_method == 'reg':\n",
    "            pos_s_Vo_emb = text_embs['subject'] * (pos_verb_mat_obj @ text_embs['object'])\n",
    "            pos_sV_o_emb = (text_embs['subject'] @ pos_verb_mat_subj) * text_embs['object']\n",
    "            neg_s_Vo_emb = text_embs['subject'] * (neg_verb_mat_obj @ text_embs['object'])\n",
    "            neg_sV_o_emb = (text_embs['subject'] @ neg_verb_mat_subj) * text_embs['object']\n",
    "        else:\n",
    "            pos_s_Vo_emb = text_embs['subject'] * (pos_verb_mat @ text_embs['object'])\n",
    "            pos_sV_o_emb = (text_embs['subject'] @ pos_verb_mat) * text_embs['object']\n",
    "            neg_s_Vo_emb = text_embs['subject'] * (neg_verb_mat @ text_embs['object'])\n",
    "            neg_sV_o_emb = (text_embs['subject'] @ neg_verb_mat) * text_embs['object']\n",
    "        \n",
    "        pos_add = pos_s_Vo_emb + pos_sV_o_emb\n",
    "        neg_add = neg_s_Vo_emb + neg_sV_o_emb\n",
    "        \n",
    "        if options.get('normalize_compo', False):\n",
    "            pos_s_Vo_emb = normalise(pos_s_Vo_emb)\n",
    "            pos_sV_o_emb = normalise(pos_sV_o_emb)\n",
    "            neg_s_Vo_emb = normalise(neg_s_Vo_emb)\n",
    "            neg_sV_o_emb = normalise(neg_sV_o_emb)\n",
    "            pos_add = normalise(pos_add)\n",
    "            neg_add = normalise(neg_add)\n",
    "        \n",
    "        pos_vec_copy_subj = text_embs['pos_caption'] + pos_s_Vo_emb\n",
    "        pos_vec_copy_obj = text_embs['pos_caption'] + pos_sV_o_emb\n",
    "        pos_vec_copy_add = text_embs['pos_caption'] + pos_add\n",
    "        neg_vec_copy_subj = text_embs['neg_caption'] + neg_s_Vo_emb\n",
    "        neg_vec_copy_obj = text_embs['neg_caption'] + neg_sV_o_emb\n",
    "        neg_vec_copy_add = text_embs['neg_caption'] + neg_add\n",
    "        \n",
    "        if options.get('cos_sim', False):\n",
    "            f = lambda a, b: np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        else:\n",
    "            f = lambda a, b: np.dot(a, b)\n",
    "\n",
    "        scores['pos_clip'].append(f(text_embs['pos_caption'], image_embs))\n",
    "        scores['pos_copy_subj'].append(f(pos_vec_copy_subj, image_embs))\n",
    "        scores['pos_copy_obj'].append(f(pos_vec_copy_obj, image_embs))\n",
    "        scores['pos_copy_add'].append(f(pos_vec_copy_add, image_embs))\n",
    "        scores['pos_copy_subj_alone'].append(f(pos_s_Vo_emb, image_embs))\n",
    "        scores['pos_copy_obj_alone'].append(f(pos_sV_o_emb, image_embs))\n",
    "\n",
    "        scores['neg_clip'].append(f(text_embs['neg_caption'], image_embs))\n",
    "        scores['neg_copy_subj'].append(f(neg_vec_copy_subj, image_embs))\n",
    "        scores['neg_copy_obj'].append(f(neg_vec_copy_obj, image_embs))\n",
    "        scores['neg_copy_add'].append(f(neg_vec_copy_add, image_embs))\n",
    "        scores['neg_copy_subj_alone'].append(f(neg_s_Vo_emb, image_embs))\n",
    "        scores['neg_copy_obj_alone'].append(f(neg_sV_o_emb, image_embs))\n",
    "\n",
    "    # add the scores to the dataframe\n",
    "    for k, v in scores.items():\n",
    "        data[k] = v\n",
    "        \n",
    "        # add the difference scores to the dataframe\n",
    "        # a positive value indicates that a higher similarity score was given to the positive caption\n",
    "        if k.startswith('pos'):\n",
    "            data[k.replace('pos', 'diff')] = np.array(scores[k]) - np.array(scores[k.replace('pos', 'neg')])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "    normalize_caption: True\n",
      "    normalize_parts: True\n",
      "    normalize_compo: False\n",
      "    normalize_rel: True\n",
      "    normalize_kron: True\n",
      "    normalize_reg: True\n",
      "    cos_sim: False\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25455f74a0d04e4bbc2f9b111f155cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " method: kron\n",
      "    Copy-Subj: 59.53\n",
      "    Copy-Obj: 58.53\n",
      "    Copy-Add: 60.41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e933848aeaa4c70b8cabc557f2b3a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " method: rel\n",
      "    Copy-Subj: 58.80\n",
      "    Copy-Obj: 56.62\n",
      "    Copy-Add: 57.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55235eac8994205a53796a2eae9325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " method: reg\n",
      "    Copy-Subj: 58.49\n",
      "    Copy-Obj: 52.56\n",
      "    Copy-Add: 59.53\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'normalize_caption': True,\n",
    "    'normalize_parts': True,\n",
    "    'normalize_compo': False,\n",
    "    'normalize_rel': True,\n",
    "    'normalize_kron': True,\n",
    "    'normalize_reg': True,\n",
    "    'cos_sim': False\n",
    "}\n",
    "\n",
    "print(f\"Options:\")\n",
    "for k, v in options.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "print(\"Accuracy:\")\n",
    "result = matrix_methods(data, 'kron', **options)\n",
    "print(f\" method: kron\")\n",
    "print(f\"    Copy-Subj: {report_scores(result['pos_copy_subj'], result['neg_copy_subj'])*100:.2f}\")\n",
    "print(f\"    Copy-Obj: {report_scores(result['pos_copy_obj'], result['neg_copy_obj'])*100:.2f}\")\n",
    "print(f\"    Copy-Add: {report_scores(result['pos_copy_add'], result['neg_copy_add'])*100:.2f}\")\n",
    "\n",
    "result = matrix_methods(data, 'rel', **options)\n",
    "print(f\" method: rel\")\n",
    "print(f\"    Copy-Subj: {report_scores(result['pos_copy_subj'], result['neg_copy_subj'])*100:.2f}\")\n",
    "print(f\"    Copy-Obj: {report_scores(result['pos_copy_obj'], result['neg_copy_obj'])*100:.2f}\")\n",
    "print(f\"    Copy-Add: {report_scores(result['pos_copy_add'], result['neg_copy_add'])*100:.2f}\")\n",
    "\n",
    "result = matrix_methods(data, 'reg', **options)\n",
    "print(f\" method: reg\")\n",
    "print(f\"    Copy-Subj: {report_scores(result['pos_copy_subj'], result['neg_copy_subj'])*100:.2f}\")\n",
    "print(f\"    Copy-Obj: {report_scores(result['pos_copy_obj'], result['neg_copy_obj'])*100:.2f}\")\n",
    "print(f\"    Copy-Add: {report_scores(result['pos_copy_add'], result['neg_copy_add'])*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_methods(data, **options):\n",
    "    \"\"\"\n",
    "    The available options are:\n",
    "    \n",
    "    - normalize_caption: whether to normalise the caption embeddings\n",
    "    - normalize_parts: whether to normalise the embeddings of the parts of the sentence\n",
    "    - normalize_compo: whether to normalise the entire compositional part.\n",
    "    \n",
    "    - cos_sim: whether to use cosine similarity or dot product for the calculation of the scores.\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        text_data = {'pos_verb': row['pos_verb'], 'neg_verb': row['neg_verb'], \n",
    "                     'subject': row['subject'], 'object': row['object'], \n",
    "                     'pos_caption': row['pos_caption'], 'neg_caption': row['neg_caption']}\n",
    "        text_embs = {k: encode_text(v) for k, v in text_data.items()}\n",
    "        image_embs = encode_image(row['image_id'])\n",
    "        \n",
    "        if options.get('normalize_caption', False):\n",
    "            text_embs['pos_caption'] = normalise(text_embs['pos_caption'])\n",
    "            text_embs['neg_caption'] = normalise(text_embs['neg_caption'])\n",
    "            \n",
    "        if options.get('normalize_parts', False):\n",
    "            text_embs['pos_verb'] = normalise(text_embs['pos_verb'])\n",
    "            text_embs['neg_verb'] = normalise(text_embs['neg_verb'])\n",
    "            text_embs['subject'] = normalise(text_embs['subject'])\n",
    "            text_embs['object'] = normalise(text_embs['object'])\n",
    "\n",
    "        pos_sum = text_embs['subject'] + text_embs['pos_verb'] + text_embs['object']\n",
    "        neg_sum = text_embs['subject'] + text_embs['neg_verb'] + text_embs['object']\n",
    "        pos_mult = text_embs['subject'] * text_embs['pos_verb'] * text_embs['object']\n",
    "        neg_mult = text_embs['subject'] * text_embs['neg_verb'] * text_embs['object']\n",
    "\n",
    "        if options.get('normalize_compo', False):\n",
    "            pos_sum = normalise(pos_sum)\n",
    "            neg_sum = normalise(neg_sum)\n",
    "            pos_mult = normalise(pos_mult)\n",
    "            neg_mult = normalise(neg_mult) \n",
    "\n",
    "        pos_vec_add = text_embs['pos_caption'] + pos_sum\n",
    "        pos_vec_mult = text_embs['pos_caption'] + pos_mult\n",
    "        neg_vec_add = text_embs['neg_caption'] + neg_sum\n",
    "        neg_vec_mult = text_embs['neg_caption'] + neg_mult\n",
    "        \n",
    "        if options.get('cos_sim', False):\n",
    "            f = lambda a, b: np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        else:\n",
    "            f = lambda a, b: np.dot(a, b)\n",
    "        \n",
    "        scores['pos_add'].append(f(pos_vec_add, image_embs))\n",
    "        scores['pos_mult'].append(f(pos_vec_mult, image_embs))\n",
    "        scores['neg_add'].append(f(neg_vec_add, image_embs))\n",
    "        scores['neg_mult'].append(f(neg_vec_mult, image_embs))\n",
    "        scores['pos_clip'].append(f(text_embs['pos_caption'], image_embs))\n",
    "        scores['neg_clip'].append(f(text_embs['neg_caption'], image_embs))\n",
    "        \n",
    "    # add the scores to the dataframe\n",
    "    for k, v in scores.items():\n",
    "        data[k] = v\n",
    "        \n",
    "        # add the difference scores to the dataframe\n",
    "        # a positive value indicates that a higher similarity score was given to the positive caption\n",
    "        if k.startswith('pos'):\n",
    "            data[k.replace('pos', 'diff')] = np.array(scores[k]) - np.array(scores[k.replace('pos', 'neg')])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "    normalize_caption: True\n",
      "    normalize_parts: True\n",
      "    normalize_compo: False\n",
      "    cos_sim: False\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a808840cb31b4449b67096a1472ded13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Add: 60.00\n",
      "    Mult: 57.83\n",
      "    Clip: 57.27\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'normalize_caption': True,\n",
    "    'normalize_parts': True,\n",
    "    'normalize_compo': False,\n",
    "    'cos_sim': False\n",
    "}\n",
    "\n",
    "print(f\"Options:\")\n",
    "for k, v in options.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "print(\"Accuracy:\")\n",
    "result = vector_methods(data, **options)\n",
    "print(f\"    Add: {report_scores(result['pos_add'], result['neg_add'])*100:.2f}\")\n",
    "print(f\"    Mult: {report_scores(result['pos_mult'], result['neg_mult'])*100:.2f}\")\n",
    "print(f\"    Clip: {report_scores(result['pos_clip'], result['neg_clip'])*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "    normalize_caption: True\n",
      "    normalize_parts: True\n",
      "    normalize_compo: False\n",
      "    normalize_rel: True\n",
      "    normalize_kron: True\n",
      "    normalize_reg: True\n",
      "    cos_sim: False\n",
      " method: kron\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2063837c694a29858605a6a036e073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted accuracy: 60.41\n",
      "Best accuracy: 66.47\n",
      "Best alpha, beta:\n",
      "    (9.800000000000008, 1.0)\n",
      " method: rel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d5e8438a5f448db119716d55128ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted accuracy: 57.85\n",
      "Best accuracy: 65.47\n",
      "Best alpha, beta:\n",
      "    (10.000000000000007, 1.0)\n",
      " method: reg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0d8620234045a19fe164a65da31c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted accuracy: 59.53\n",
      "Best accuracy: 62.90\n",
      "Best alpha, beta:\n",
      "    (4.700000000000003, 1.8000000000000007)\n"
     ]
    }
   ],
   "source": [
    "def report_best_alpha_beta(result):\n",
    "    accs = dict()\n",
    "    for alpha in np.arange(1, 10.1, 0.1):\n",
    "        for beta in np.arange(1, 10.1, 0.1):\n",
    "            pos = result['pos_clip'] + alpha * result['pos_copy_subj_alone'] + beta * result['pos_copy_obj_alone']\n",
    "            neg = result['neg_clip'] + alpha * result['neg_copy_subj_alone'] + beta * result['neg_copy_obj_alone']\n",
    "            acc = report_scores(pos, neg)\n",
    "            accs[(alpha, beta)] = acc\n",
    "    acc_unweighted = report_scores(result['pos_copy_add'], result['neg_copy_add'])\n",
    "\n",
    "    # fine the best acc\n",
    "    best_acc = max(accs.values())\n",
    "    print(f\"Unweighted accuracy: {acc_unweighted*100:.2f}\")\n",
    "    print(f\"Best accuracy: {best_acc*100:.2f}\")\n",
    "    print(f\"Best alpha, beta:\")\n",
    "    for k, v in accs.items():\n",
    "        if v == best_acc:\n",
    "            print(f\"    {k}\")\n",
    "            \n",
    "options = {\n",
    "    'normalize_caption': True,\n",
    "    'normalize_parts': True,\n",
    "    'normalize_compo': False,\n",
    "    'normalize_rel': True,\n",
    "    'normalize_kron': True,\n",
    "    'normalize_reg': True,\n",
    "    'cos_sim': False\n",
    "}\n",
    "print(f\"Options:\")\n",
    "for k, v in options.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "print(f\" method: kron\")\n",
    "result = matrix_methods(data, 'kron', **options)\n",
    "report_best_alpha_beta(result)\n",
    "\n",
    "print(f\" method: rel\")\n",
    "result = matrix_methods(data, 'rel', **options)\n",
    "report_best_alpha_beta(result)\n",
    "\n",
    "print(f\" method: reg\")\n",
    "result = matrix_methods(data, 'reg', **options)\n",
    "report_best_alpha_beta(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnlp31013m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
