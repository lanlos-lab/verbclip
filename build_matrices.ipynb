{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook build matrix representation of transitive verbs using the vector embeddings of its arguments, that is, the subject and the object.\n",
    "\n",
    "Preparation before running this notebook:\n",
    "1. Extract a \n",
    "2. Run the notebook `encode_texts.ipynb` to precompute the embeddings of the texts for efficiency. You can skip this step if you want to run the notebook with on the fly embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# set the path to the cache file, which should be a pickle file\n",
    "# of a dictionary where the keys are the strings and the values\n",
    "# are embeddings\n",
    "# if you have not generated the cache file, leave it as None\n",
    "emb_cache_path = None\n",
    "\n",
    "# load the clip model\n",
    "# comment out the following 2 lines if you are confident that\n",
    "# everything string is in the cache file\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "model_name = \"32\" # 32 for ViT-B/32 and 14 for ViT-L/14\n",
    "\n",
    "# load the cache file\n",
    "if emb_cache_path is not None:\n",
    "    with open(emb_cache_path, \"rb\") as f:\n",
    "        text_emb_cache = pickle.load(f)\n",
    "else:\n",
    "    text_emb_cache = dict()\n",
    "\n",
    "# make sure that the embeddings are float32 numpy arrays \n",
    "text_emb_cache = {k: v.astype(np.float32) for k, v in text_emb_cache.items()}\n",
    "\n",
    "def get_text_emb(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): text to be embedded\n",
    "    \n",
    "    Returns:\n",
    "        emb (np.array): text embedding of shape (d, ) where d is the dimension of the embedding\n",
    "    \"\"\"\n",
    "    if text in text_emb_cache:\n",
    "        return text_emb_cache[text]\n",
    "    inputs = clip.tokenize(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(inputs).cpu().numpy().flatten()\n",
    "    text_emb_cache[text] = text_features\n",
    "    return text_features\n",
    "\n",
    "def normalize(emb):\n",
    "    return emb / np.linalg.norm(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rel_matrices(subj_list, obj_list, norm_vec=False, norm_num=False):\n",
    "    \"\"\"\n",
    "    Build the matrix presentation of the verb given the subject-verb-object triples\n",
    "    found in a corpus. Note that the verb itself is not used in this construction.\n",
    "    \n",
    "    The relational matrix is defined as the sum of the outer product of the embeddings\n",
    "    of the subject and object of each triple:\n",
    "    M = \\sum_{k=1}^{n} outer_product(emb(subj_list[k]), emb(obj_list[k])).\n",
    "    The outer product takes two vectors of size d and returns a matrix of size d x d.\n",
    "\n",
    "    Args:\n",
    "        subj_list (list): list of subject nouns in the triples, which has the same length as obj_list\n",
    "        obj_list (list): list of object nouns in the triples, which has the same length as subj_list\n",
    "        norm_vec (bool): whether to normalize the vectors of the subjects and objects\n",
    "        norm_num (bool): whether to normalize the matrix by the number of triples\n",
    "    \n",
    "    Returns:\n",
    "        matrix (np.array): the relational matrix representation of the verb\n",
    "        \n",
    "    Example:\n",
    "        >>> # verb is 'chase' and the triples are\n",
    "        >>> # 'dog chase ball', 'cat chase mouse', 'police chase car'\n",
    "        >>> subj_list = ['dog', 'cat', 'police']\n",
    "        >>> obj_list = ['ball', 'mouse', 'car']\n",
    "        >>> matrix = build_rel_matrices(subj_list, obj_list)\n",
    "    \"\"\"\n",
    "    if len(subj_list) != len(obj_list):\n",
    "        raise ValueError('subj_list and obj_list must have the same length')\n",
    "    num_triples = len(subj_list)\n",
    "   \n",
    "    if norm_vec: \n",
    "        post_process = normalize\n",
    "    else:\n",
    "        post_process = lambda x: x\n",
    "\n",
    "    subj_arr = np.array([post_process(get_text_emb(subj)) for subj in subj_list])\n",
    "    obj_arr = np.array([post_process(get_text_emb(obj)) for obj in obj_list])\n",
    "        \n",
    "    matrix = subj_arr.T @ obj_arr\n",
    "    if norm_num:\n",
    "        matrix /= num_triples\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression-based methods for building verb matrices\n",
    "The idea is to find a verb matrix `V` such that, when multiplied with the vector of a noun, it gives the vector of the verb-noun pair.\n",
    "There are two cases:\n",
    "\n",
    "- subject-verb, e.g. cat eats\n",
    "- verb-object, e.g. eats fish\n",
    "    \n",
    "For a given verb, we gather subject-verb-object triples from a corpus.\n",
    "We would like the following to hold (approximately):\n",
    "```\n",
    "emb(s) @ V  == emb(sv)\n",
    "V @ emb(o) == emb(vo)\n",
    "```\n",
    "    \n",
    "where `V` is the verb matrix, `emb(s)` is the vector of the subject, `emb(o)` is the vector of the object, `emb(sv)` is the vector of the subject-verb pair, and `emb(vo)` is the vector of the verb-object pair.\n",
    "\n",
    "Consider a vectorized form of the subject-verb case with `n` subject-verb-object triples and `d` the dimension of the vectors:\n",
    "``` \n",
    "S @ V == SV\n",
    "```\n",
    "where S is a matrix of shape (n, d), V is a matrix of shape (d, d), and SV is a matrix of shape (n, d).\n",
    "Here `S[k,:]` gives the vector of the k-th subject, and `SV[k,:]` gives the vector of the k-th subject-verb pair.\n",
    "\n",
    "An exact solution for V is most likely not possible, but we can find the best approximation using the least squares method.\n",
    "The idea is to minimize the following loss function:\n",
    "```\n",
    "L = ||S @ V - SV||^2\n",
    "```\n",
    "where `||.||` is the Frobenius norm, that is, the sum of the squared elements of the matrix.\n",
    "\n",
    "The solution is given by the Penrose-Moore pseudo-inverse of S:\n",
    "```\n",
    "V = S^+ @ SV\n",
    "```\n",
    "where `S^+` is the pseudo-inverse of S, which can be obtained in numpy with `np.linalg.pinv`.\n",
    "\n",
    "<!-- For the verb-object case, we can do the same thing, but the vetorized form is slightly different: -->\n",
    "We can do the same for the verb-object case, but the verb matrix has to be transposed to make the tensor contraction work:\n",
    "```\n",
    "O @ V^T = VO\n",
    "```\n",
    "The solution is given by an extra transpose: `V = (O^+ @ VO)^T`. \n",
    "\n",
    "The transpose is necessary because the object vector is supposed to be on the right side of the verb matrix. Since we are working with a batched version of the problem, `O` is of shape (n, d). For the `d` part to interact with the verb matrix, we need to put it on the right side of `V` and transpose `V` so that the interaction is correct. This transpose is only an artifact of the batched version of the problem and carries no special meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_square(A, B):\n",
    "    \"\"\"\n",
    "    Given two matrices A and B, solve for X such that l2_norm(A @ X - B) is minimized.\n",
    "    \"\"\"\n",
    "    return np.linalg.pinv(A) @ B\n",
    "    \n",
    "def build_reg_matrices_subj(subj_list, subj_verb_list, norm_vec=False):\n",
    "    \"\"\"\n",
    "    Given a list of subjects and their corresponding subject-verb pairs,\n",
    "    returns a regression matrix V such that l2_norm(subj_arr @ V - subj_verb_arr) is minimized.\n",
    "    where subj_arr and subj_verb_arr have shape (num_triples, emb_dim).\n",
    "\n",
    "    Note:\n",
    "    The solution can be found by taking the pseudo-inverse of subj_arr and multiplying it with subj_verb_arr.\n",
    "    subj_arr @ V = subj_verb_arr => V = pinv(subj_arr) @ subj_verb_arr\n",
    "\n",
    "    Args:\n",
    "        subj_list (list): list of subject nouns\n",
    "        subj_verb_list (list): list of subject-verb pairs\n",
    "        norm_vec (bool): whether to normalize the embeddings of the subjects and subject-verb pairs\n",
    "    \n",
    "    Returns:\n",
    "        matrix (np.array): the regression matrix representation of the verb\n",
    "    \"\"\"\n",
    "    if norm_vec:\n",
    "        post_process = normalize\n",
    "    else:\n",
    "        post_process = lambda x: x\n",
    "        \n",
    "    subj_arr = np.array([post_process(get_text_emb(subj)) for subj in subj_list])\n",
    "    subj_verb_arr = np.array([post_process(get_text_emb(subj_verb)) for subj_verb in subj_verb_list])\n",
    "\n",
    "    # Approximate solution to S @ V = SV\n",
    "    # V = pinv(S) @ SV\n",
    "    matrix = solve_least_square(subj_arr, subj_verb_arr) \n",
    "    return matrix\n",
    "    \n",
    "def build_reg_matrices_obj(verb_obj_list, obj_list, norm_vec=False):\n",
    "    \"\"\"\n",
    "    Given a list of objects and their corresponding verb-object pairs,\n",
    "    returns a regression matrix V such that l2_norm(obj_arr @ V^T - verb_obj_arr) is minimized.\n",
    "    \n",
    "    Note:\n",
    "    The solution can be found by taking the pseudo-inverse of obj_arr and multiplying it with verb_obj_arr.\n",
    "    obj_arr @ V^T = verb_obj_arr => V = pinv(obj_arr) @ verb_obj_arr\n",
    "\n",
    "    Args:\n",
    "        subj_verb_list (list): list of subject-verb pairs\n",
    "        obj_list (list): list of object nouns\n",
    "        norm_vec: whether to normalize the embeddings of the subject-verb pairs and objects\n",
    "    \n",
    "    Returns:\n",
    "        matrix (np.array): the regression matrix representation of the verb\n",
    "    \"\"\"\n",
    "    if norm_vec:\n",
    "        post_process = normalize\n",
    "    else:\n",
    "        post_process = lambda x: x    \n",
    "        \n",
    "    verb_obj_arr = np.array([post_process(get_text_emb(verb_obj)) for verb_obj in verb_obj_list])\n",
    "    obj_arr = np.array([post_process(get_text_emb(obj)) for obj in obj_list])\n",
    "        \n",
    "    # Approximate solution to O @ V^T = VO\n",
    "    matrix = np.transpose(solve_least_square(obj_arr, verb_obj_arr))\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_svo_path = Path('experiments/svo_probes/cleaned_svo')\n",
    "\n",
    "rel_matrices = dict()\n",
    "rel_matrices_norm = dict()\n",
    "reg_subj_matrices = dict()\n",
    "reg_obj_matrices = dict()\n",
    "\n",
    "files = list(cleaned_svo_path.glob('*.csv'))\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv(f)\n",
    "    if df.empty:\n",
    "        print(f'{f.stem} is empty')\n",
    "        continue\n",
    "    subj_list = df['subject_text']\n",
    "    obj_list = df['object_text']\n",
    "    verb_list = df['verb_text']\n",
    "    \n",
    "    subj_verb_list = subj_list + ' ' + verb_list\n",
    "    verb_obj_list = verb_list + ' ' + obj_list\n",
    "    \n",
    "    rel_matrix = build_rel_matrices(subj_list, obj_list, norm_vec=True, norm_num=False)\n",
    "    rel_matrix_norm = build_rel_matrices(subj_list, obj_list, norm_vec=True, norm_num=True)\n",
    "    reg_subj_matrix = build_reg_matrices_subj(subj_list, subj_verb_list, norm_vec=True)\n",
    "    reg_obj_matrix = build_reg_matrices_obj(verb_obj_list, obj_list, norm_vec=True)\n",
    "    \n",
    "    rel_matrices[f.stem] = rel_matrix\n",
    "    rel_matrices_norm[f.stem] = rel_matrix_norm\n",
    "    reg_subj_matrices[f.stem] = reg_subj_matrix\n",
    "    reg_obj_matrices[f.stem] = reg_obj_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the matrices\n",
    "rel_matrices_path = cleaned_svo_path.parent / f'embeddings/rel_matrices_{model_name}.pkl'\n",
    "rel_matrices_norm_path = cleaned_svo_path.parent / f'embeddings/rel_matrices_norm_{model_name}.pkl'\n",
    "reg_subj_matrices_path = cleaned_svo_path.parent / f'embeddings/reg_subj_matrices_{model_name}.pkl'\n",
    "reg_obj_matrices_path = cleaned_svo_path.parent / f'embeddings/reg_obj_matrices_{model_name}.pkl'\n",
    "\n",
    "rel_matrices_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "rel_matrices_norm_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "reg_subj_matrices_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "reg_obj_matrices_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pickle.dump(rel_matrices, open(rel_matrices_path, 'wb'))\n",
    "pickle.dump(rel_matrices_norm, open(rel_matrices_norm_path, 'wb'))\n",
    "pickle.dump(reg_subj_matrices, open(reg_subj_matrices_path, 'wb'))\n",
    "pickle.dump(reg_obj_matrices, open(reg_obj_matrices_path, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnlp31013m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
